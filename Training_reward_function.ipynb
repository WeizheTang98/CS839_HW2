{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfPcedXLwiKv",
        "outputId": "f2c36fa0-01c4-43f9-c3d0-f21d1b1fa95a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-90d7e7d3dd8b>:311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(save_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from txtcnn.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Train Loss: 0.3685, Valid Loss: 0.5486\n",
            "Accuracy: 0.5834\n",
            "New best model saved with validation loss: 0.5486 with acc 0.5834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/50], Train Loss: 0.3689, Valid Loss: 0.5693\n",
            "Accuracy: 0.5965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/50], Train Loss: 0.3664, Valid Loss: 0.5663\n",
            "Accuracy: 0.5936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/50], Train Loss: 0.3668, Valid Loss: 0.5630\n",
            "Accuracy: 0.5924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/50], Train Loss: 0.3667, Valid Loss: 0.5548\n",
            "Accuracy: 0.5829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/50], Train Loss: 0.3645, Valid Loss: 0.5407\n",
            "Accuracy: 0.5691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/50], Train Loss: 0.3669, Valid Loss: 0.5671\n",
            "Accuracy: 0.5916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/50], Train Loss: 0.3631, Valid Loss: 0.5624\n",
            "Accuracy: 0.59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/50], Train Loss: 0.3656, Valid Loss: 0.5542\n",
            "Accuracy: 0.5844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50], Train Loss: 0.3648, Valid Loss: 0.5667\n",
            "Accuracy: 0.5938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/50], Train Loss: 0.3628, Valid Loss: 0.5501\n",
            "Accuracy: 0.5822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/50], Train Loss: 0.3648, Valid Loss: 0.5595\n",
            "Accuracy: 0.589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/50], Train Loss: 0.3639, Valid Loss: 0.5541\n",
            "Accuracy: 0.5839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/50], Train Loss: 0.3616, Valid Loss: 0.5565\n",
            "Accuracy: 0.5925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/50], Train Loss: 0.3624, Valid Loss: 0.5724\n",
            "Accuracy: 0.5951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/50], Train Loss: 0.3632, Valid Loss: 0.5475\n",
            "Accuracy: 0.5826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/50], Train Loss: 0.3615, Valid Loss: 0.5486\n",
            "Accuracy: 0.5806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/50], Train Loss: 0.3614, Valid Loss: 0.5556\n",
            "Accuracy: 0.5879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/50], Train Loss: 0.3625, Valid Loss: 0.5752\n",
            "Accuracy: 0.5982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/50], Train Loss: 0.3592, Valid Loss: 0.5710\n",
            "Accuracy: 0.5974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/50], Train Loss: 0.3606, Valid Loss: 0.5531\n",
            "Accuracy: 0.5773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/50], Train Loss: 0.3622, Valid Loss: 0.5606\n",
            "Accuracy: 0.588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/50], Train Loss: 0.3596, Valid Loss: 0.5760\n",
            "Accuracy: 0.5963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/50], Train Loss: 0.3620, Valid Loss: 0.5650\n",
            "Accuracy: 0.5874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/50], Train Loss: 0.3600, Valid Loss: 0.5464\n",
            "Accuracy: 0.5775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/50], Train Loss: 0.3586, Valid Loss: 0.5469\n",
            "Accuracy: 0.5825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/50], Train Loss: 0.3592, Valid Loss: 0.5335\n",
            "Accuracy: 0.5663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/50], Train Loss: 0.3591, Valid Loss: 0.5433\n",
            "Accuracy: 0.5716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/50], Train Loss: 0.3591, Valid Loss: 0.5468\n",
            "Accuracy: 0.5761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/50], Train Loss: 0.3591, Valid Loss: 0.5418\n",
            "Accuracy: 0.5793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [31/50], Train Loss: 0.3578, Valid Loss: 0.5893\n",
            "Accuracy: 0.6042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [32/50], Train Loss: 0.3580, Valid Loss: 0.5489\n",
            "Accuracy: 0.5814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [33/50], Train Loss: 0.3596, Valid Loss: 0.5625\n",
            "Accuracy: 0.591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [34/50], Train Loss: 0.3574, Valid Loss: 0.5701\n",
            "Accuracy: 0.5956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [35/50], Train Loss: 0.3566, Valid Loss: 0.5612\n",
            "Accuracy: 0.5892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [36/50], Train Loss: 0.3552, Valid Loss: 0.5638\n",
            "Accuracy: 0.5943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [37/50], Train Loss: 0.3562, Valid Loss: 0.5413\n",
            "Accuracy: 0.5764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [38/50], Train Loss: 0.3539, Valid Loss: 0.5575\n",
            "Accuracy: 0.5911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [39/50], Train Loss: 0.3570, Valid Loss: 0.5636\n",
            "Accuracy: 0.5945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [40/50], Train Loss: 0.3567, Valid Loss: 0.5661\n",
            "Accuracy: 0.5891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [41/50], Train Loss: 0.3561, Valid Loss: 0.5820\n",
            "Accuracy: 0.6012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [42/50], Train Loss: 0.3570, Valid Loss: 0.5647\n",
            "Accuracy: 0.5867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [43/50], Train Loss: 0.3542, Valid Loss: 0.5599\n",
            "Accuracy: 0.5868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [44/50], Train Loss: 0.3550, Valid Loss: 0.5402\n",
            "Accuracy: 0.5704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [45/50], Train Loss: 0.3570, Valid Loss: 0.5771\n",
            "Accuracy: 0.5949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [46/50], Train Loss: 0.3583, Valid Loss: 0.5585\n",
            "Accuracy: 0.5766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [47/50], Train Loss: 0.3544, Valid Loss: 0.5556\n",
            "Accuracy: 0.5778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [48/50], Train Loss: 0.3528, Valid Loss: 0.5632\n",
            "Accuracy: 0.5845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [49/50], Train Loss: 0.3538, Valid Loss: 0.5617\n",
            "Accuracy: 0.5817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/50], Train Loss: 0.3537, Valid Loss: 0.5562\n",
            "Accuracy: 0.5805\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "# Load meta.pkl data\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Config:\n",
        "    model_name = 'txtcnn'\n",
        "    epoch = 50\n",
        "    embedding_size = 128\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define custom dataset class for PyTorch\n",
        "class RewardDataset(Dataset):\n",
        "    def __init__(self, dataframe, stoi, vocab_size):\n",
        "        self.data = dataframe\n",
        "        self.stoi = stoi\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the prompt, response, and reward\n",
        "        prompt = self.data.iloc[idx][\"prompt\"]\n",
        "        true_response = self.data.iloc[idx][\"true_response\"]\n",
        "        first_false_response = self.data.iloc[idx][\"first_false_response\"]\n",
        "        second_false_response = self.data.iloc[idx][\"second_false_response\"]\n",
        "        adversarial_false_response = self.data.iloc[idx][\"adversarial_false_response\"]\n",
        "\n",
        "\n",
        "        # reward = self.data.iloc[idx][\"Reward\"]\n",
        "\n",
        "        # Tokenize prompt and response (convert characters to integer tokens)\n",
        "        prompt_tokens = self.tokenizer(prompt)\n",
        "        true_response_tokens = self.tokenizer(true_response)\n",
        "        first_false_response_tokens = self.tokenizer(first_false_response)\n",
        "        second_false_response_tokens = self.tokenizer(second_false_response)\n",
        "        adversarial_false_response_tokens = self.tokenizer(adversarial_false_response)\n",
        "\n",
        "        # Pad or truncate tokens to fixed size (e.g., 1024 tokens)\n",
        "        prompt_tokens = prompt_tokens[:64] + [0] * (64 - len(prompt_tokens))  # Padding\n",
        "        true_response_tokens = true_response_tokens[:64] + [0] * (64 - len(true_response_tokens))  # Padding\n",
        "        first_false_response_tokens = first_false_response_tokens[:64] + [0] * (64 - len(first_false_response_tokens))  # Padding\n",
        "        second_false_response_tokens = second_false_response_tokens[:64] + [0] * (64 - len(second_false_response_tokens))  # Padding\n",
        "        adversarial_false_response_tokens = adversarial_false_response_tokens[:64] + [0] * (64 - len(adversarial_false_response_tokens))  # Padding\n",
        "\n",
        "\n",
        "        # return torch.tensor(prompt_tokens), torch.tensor(response_tokens), torch.tensor([reward], dtype=torch.float32)\n",
        "        return torch.tensor(prompt_tokens), torch.tensor(true_response_tokens), torch.tensor(first_false_response_tokens), torch.tensor(second_false_response_tokens), torch.tensor(adversarial_false_response_tokens)\n",
        "\n",
        "    def tokenizer(self, text):\n",
        "        \"\"\"Tokenize input text using stoi (string-to-index).\"\"\"\n",
        "        return [self.stoi.get(c, 0) for c in text]  # Map characters to their respective token ids\n",
        "\n",
        "# Define the neural network model for regression\n",
        "class RewardRegressionModel(nn.Module):\n",
        "    def __init__(self, vocab_size=86, embedding_dim=768, hidden_dim=512, output_dim=1):\n",
        "        super(RewardRegressionModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.fc1 = nn.Linear(64 * embedding_dim * 2, hidden_dim)  # Multiply by 2 for prompt + response\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, prompt, response):\n",
        "        # Embed the input sequences (prompt and response)\n",
        "        prompt_emb = self.embedding(prompt).view(prompt.size(0), -1)  # Flatten to (batch_size, 1024 * embedding_dim)\n",
        "        response_emb = self.embedding(response).view(response.size(0), -1)\n",
        "\n",
        "        # Concatenate the embeddings\n",
        "        combined = torch.cat((prompt_emb, response_emb), dim=1)\n",
        "\n",
        "        # Pass through fully connected layers\n",
        "        x = self.fc1(combined)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Seq2SeqRewardModel(nn.Module):\n",
        "    def __init__(self, vocab_size=86, embedding_dim=128, hidden_dim=256):\n",
        "        super(Seq2SeqRewardModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.decoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)  # Output 1 for binary classification\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, prompt, response):\n",
        "        prompt_emb = self.embedding(prompt)\n",
        "        response_emb = self.embedding(response)\n",
        "\n",
        "        # Pass through encoder and decoder LSTMs\n",
        "        _, (encoder_hidden, _) = self.encoder(prompt_emb)\n",
        "        decoder_output, _ = self.decoder(response_emb, (encoder_hidden, encoder_hidden))\n",
        "\n",
        "        # Use the hidden state to predict the reward (label 1 or 0)\n",
        "        output = self.fc(decoder_output[:, -1, :])  # Get output for the last timestep\n",
        "        output = self.sigmoid(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class GlobalMaxPool1d(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GlobalMaxPool1d, self).__init__()\n",
        "    def forward(self, x):\n",
        "         # x shape: (batch_size, channel, seq_len)\n",
        "        return F.max_pool1d(x, kernel_size=x.shape[2]) # shape: (batch_size, channel, 1)\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size = 86, embedding_dim = 768, kernel_sizes = [3,5,7,9,11], num_channels = [512,512,512,512,512]):\n",
        "        super(TextCNN, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)  # embedding之后的shape: torch.Size([200, 8, 300])\n",
        "        # self.word_embeddings = self.word_embeddings.from_pretrained(vectors, freeze=False)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.decoder = nn.Linear(sum(num_channels), 1)\n",
        "        # 时序最大池化层没有权重，所以可以共用一个实例\n",
        "        self.pool = GlobalMaxPool1d()\n",
        "        self.convs = nn.ModuleList()  # 创建多个一维卷积层\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        for c, k in zip(num_channels, kernel_sizes):\n",
        "            self.convs.append(nn.Conv1d(in_channels = embedding_dim,\n",
        "                                        out_channels = c,\n",
        "                                        kernel_size = k))\n",
        "\n",
        "    def forward(self, prompt, response):\n",
        "        prompt_emb = self.word_embeddings(prompt)\n",
        "        response_emb = self.word_embeddings(response)\n",
        "        # print(embeds.shape)\n",
        "        embeds = torch.cat((prompt_emb, response_emb), dim=1)\n",
        "        embeds = embeds.permute(0, 2, 1)\n",
        "        # 对于每个一维卷积层，在时序最大池化后会得到一个形状为(批量大小, 通道大小, 1)的\n",
        "        # Tensor。使用flatten函数去掉最后一维，然后在通道维上连结\n",
        "        # print(embeds)\n",
        "        encoding = torch.cat([self.pool(F.relu(conv(embeds))).squeeze(-1) for conv in self.convs], dim=1)\n",
        "        # 应用丢弃法后使用全连接层得到输出\n",
        "        outputs = self.decoder(self.dropout(encoding))\n",
        "        outputs = self.sigmoid(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=10):\n",
        "\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch [{epoch + 1}/{num_epochs}]\", leave=False)\n",
        "        # Training phase\n",
        "        for prompts,true_response,ff_response,sf_response,af_response in progress_bar:\n",
        "            prompts,true_response,ff_response,sf_response,af_response = prompts.to(device),true_response.to(device),ff_response.to(device),sf_response.to(device),af_response.to(device)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            true_outputs = model(prompts, true_response)\n",
        "            ff_outputs = model(prompts, ff_response)\n",
        "            sf_outputs = model(prompts, sf_response)\n",
        "            af_outputs = model(prompts, af_response)\n",
        "\n",
        "            true_ff_df = true_outputs - ff_outputs\n",
        "            true_sf_df = true_outputs - sf_outputs\n",
        "            true_af_df = true_outputs - af_outputs\n",
        "\n",
        "            # true_ff_df = torch.functional.F.sigmoid(true_ff_df)\n",
        "            # true_sf_df = torch.functional.F.sigmoid(true_sf_df)\n",
        "            # true_af_df = torch.functional.F.sigmoid(true_af_df)\n",
        "\n",
        "            loss_true_ff = criterion(true_ff_df, torch.ones_like(true_ff_df))\n",
        "            loss_true_sf = criterion(true_sf_df, torch.ones_like(true_sf_df))\n",
        "            loss_true_af = criterion(true_af_df, torch.ones_like(true_af_df))\n",
        "\n",
        "            loss = (loss_true_ff + loss_true_sf + loss_true_af) / 3\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        valid_loss = 0.0\n",
        "        count_right = 0\n",
        "        with torch.no_grad():\n",
        "            for prompts,true_response,ff_response,sf_response,af_response in valid_loader:\n",
        "                prompts,true_response,ff_response,sf_response,af_response = prompts.to(device),true_response.to(device),ff_response.to(device),sf_response.to(device),af_response.to(device)\n",
        "\n",
        "                true_outputs = model(prompts, true_response)\n",
        "                ff_outputs = model(prompts, ff_response)\n",
        "                sf_outputs = model(prompts, sf_response)\n",
        "                af_outputs = model(prompts, af_response)\n",
        "\n",
        "\n",
        "\n",
        "                true_ff_df = true_outputs - ff_outputs\n",
        "                true_sf_df = true_outputs - sf_outputs\n",
        "                true_af_df = true_outputs - af_outputs\n",
        "\n",
        "                # true_ff_df = torch.functional.F.sigmoid(true_ff_df)\n",
        "                # true_sf_df = torch.functional.F.sigmoid(true_sf_df)\n",
        "                # true_af_df = torch.functional.F.sigmoid(true_af_df)\n",
        "\n",
        "                loss_true_ff = criterion(true_ff_df, torch.ones_like(true_ff_df))\n",
        "                loss_true_sf = criterion(true_sf_df, torch.ones_like(true_sf_df))\n",
        "                loss_true_af = criterion(true_af_df, torch.ones_like(true_af_df))\n",
        "\n",
        "                loss = (loss_true_ff + loss_true_sf + loss_true_af) / 3\n",
        "                valid_loss += loss.item()\n",
        "\n",
        "                # count_right += true_outputs.detach().cpu().numpy().sum()\n",
        "\n",
        "                true_outputs = true_outputs.detach().cpu().numpy().tolist()\n",
        "                ff_outputs = ff_outputs.detach().cpu().numpy().tolist()\n",
        "                sf_outputs = sf_outputs.detach().cpu().numpy().tolist()\n",
        "                af_outputs = af_outputs.detach().cpu().numpy().tolist()\n",
        "\n",
        "                # print(true_outputs)\n",
        "                # print(ff_outputs)\n",
        "                # print(sf_outputs)\n",
        "                # print(af_outputs)\n",
        "\n",
        "                count_right += sum([1 if elem[0] > 0.5 else 0 for elem in true_outputs])\n",
        "                count_right += sum([1 if elem[0] < 0.5 else 0 for elem in ff_outputs])\n",
        "                count_right += sum([1 if elem[0] < 0.5 else 0 for elem in sf_outputs])\n",
        "                count_right += sum([1 if elem[0] < 0.5 else 0 for elem in af_outputs])\n",
        "\n",
        "\n",
        "        accuracy = count_right / 10000\n",
        "        avg_valid_loss = valid_loss / len(valid_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_valid_loss:.4f}\")\n",
        "        print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "        if avg_valid_loss < best_valid_loss and accuracy > best_accuracy:\n",
        "            best_valid_loss = avg_valid_loss\n",
        "            best_accuracy = accuracy\n",
        "            # Save the best model's parameters\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"New best model saved with validation loss: {best_valid_loss:.4f} with acc {accuracy:.4f}\")\n",
        "\n",
        "# Run training\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = Config()\n",
        "    # shakespeare_char = 'shakespeare_char'\n",
        "    # data_dir = os.path.join('data', shakespeare_char)\n",
        "    meta_path = 'meta.pkl'\n",
        "\n",
        "    save_path = config.model_name + \".pth\"\n",
        "\n",
        "\n",
        "\n",
        "    if os.path.exists(meta_path):\n",
        "        with open(meta_path, 'rb') as f:\n",
        "            meta = pickle.load(f)\n",
        "    # Use the previously loaded meta data\n",
        "    vocab_size = meta['vocab_size']\n",
        "    stoi = meta['stoi']  # string-to-index mapping\n",
        "    itos = meta['itos']  # index-to-string mapping\n",
        "\n",
        "    # reward_train_val = 'reward_train_val'\n",
        "    # reward_data_dir = os.path.join('data', reward_train_val)\n",
        "    reward_train_dir = 'Training_Set.csv'\n",
        "    reward_valid_dir = 'Validation_Set.csv'\n",
        "\n",
        "    train_set = pd.read_csv(reward_train_dir)\n",
        "    valid_set = pd.read_csv(reward_valid_dir)\n",
        "\n",
        "    # Create PyTorch Datasets and DataLoaders\n",
        "    train_dataset = RewardDataset(train_set, stoi, vocab_size=vocab_size)\n",
        "    valid_dataset = RewardDataset(valid_set, stoi, vocab_size=vocab_size)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Instantiate the model\n",
        "    if config.model_name == \"linear\":\n",
        "        model = RewardRegressionModel(vocab_size=vocab_size).to(device)\n",
        "    elif config.model_name == \"seq2seq\":\n",
        "        model = Seq2SeqRewardModel(vocab_size=vocab_size).to(device)\n",
        "    elif config.model_name == \"txtcnn\":\n",
        "        model = TextCNN(vocab_size=vocab_size).to(device)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model name\")\n",
        "\n",
        "    if os.path.exists(save_path):\n",
        "        model.load_state_dict(torch.load(save_path))\n",
        "        print(f\"Model loaded from {save_path}\")\n",
        "\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "    train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=config.epoch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MocIZR3iyIZD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}